{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/46135839/auto-detect-the-delimiter-in-a-csv-file-using-pd-read-csv\n",
    "import csv\n",
    "\n",
    "def get_delimiter(file_path, bytes = 4096):\n",
    "    sniffer = csv.Sniffer()\n",
    "    data = open(file_path, \"r\").read(bytes)\n",
    "    delimiter = sniffer.sniff(data).delimiter\n",
    "    return delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(file_path):\n",
    "    try:\n",
    "        for encoding in ['utf-8','latin1', 'ISO-8859-1']:\n",
    "            delimiter= get_delimiter(file_path)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, low_memory=False, encoding=encoding, delimiter=delimiter)\n",
    "                return df, True, None\n",
    "            except (UnicodeDecodeError, pd.errors.ParserError) as e:\n",
    "                pass  \n",
    "        return None, False, f\"Impossible de lire {file_path}.\"\n",
    "    except FileNotFoundError as e:\n",
    "        return None, False, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2005, 2023, 1))\n",
    "prefixes= ['caracteristiques', 'lieux', 'usagers', 'vehicules']\n",
    "dataframes= []\n",
    "\n",
    "for prefix in prefixes:\n",
    "    datasets = []\n",
    "    for year in years:\n",
    "        connector = '_' if year <= 2016 else '-'\n",
    "        file_name = f'data/raw/{prefix}{connector}{year}.csv'\n",
    "        df, success, error = read_csv_file(file_name)\n",
    "        if success:\n",
    "            key= {file_name: df}\n",
    "            datasets.append(key)\n",
    "        else:\n",
    "            print(f'{file_name} : {error}')\n",
    "    dataframes.append(datasets)\n",
    "\n",
    "print(f'Total datasets: {len(dataframes)}.')\n",
    "\n",
    "for prefix, df_list in zip(prefixes, dataframes):\n",
    "    print(f'{prefix}: {len(df_list)}.')\n",
    "\n",
    "#Correction à la main des deux fichiers pour lesquels il y avait une erreur de frappe dans le nom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recuperer le nom et le type des colonnes\n",
    "def get_df_structure(df):\n",
    "    return tuple(sorted((col, df[col].dtype) for col in df.columns))\n",
    "\n",
    "\n",
    "def group_dataframes_by_struct(dataframes):\n",
    "    #regrouper les dataframes par groupe de structures similaires\n",
    "    structure_groups = {}\n",
    "\n",
    "    for df_list in dataframes:\n",
    "        for item in df_list:\n",
    "            for filename, df in item.items():\n",
    "                structure = get_df_structure(df)\n",
    "                if structure not in structure_groups:\n",
    "                    structure_groups[structure] = []\n",
    "                structure_groups[structure].append(filename)\n",
    "    \n",
    "        #regrouper les structures\n",
    "    grouped_data = []\n",
    "    for i, (structure, files) in enumerate(structure_groups.items(), start=1):\n",
    "        unique_dtypes = set(dtype for _, dtype in structure)\n",
    "        group_info = {\n",
    "            \"group_number\": i,\n",
    "            \"num_columns\": len(structure),\n",
    "            \"unique_dtypes\": len(unique_dtypes),\n",
    "            \"structure\": structure,\n",
    "            \"files\": files\n",
    "        }\n",
    "        grouped_data.append(group_info)\n",
    "        \n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les groupes côte à côte / j'aurais pu le faire directement avec un dataframe\n",
    "def display_grouped_data(grouped_data):\n",
    "    table_headers = [\"Groupe Numéro\", \"Nombre de Colonnes\", \"Nombre de Types Uniques\", \"Structure (Colonne:Type)\", \"Fichiers\"]\n",
    "    table_data = []\n",
    "\n",
    "    for group in grouped_data:\n",
    "        group_number = group['group_number']\n",
    "        num_columns = group['num_columns']\n",
    "        unique_dtypes = group['unique_dtypes']\n",
    "        structure = group['structure']\n",
    "        files = group['files']\n",
    "        \n",
    "        structure_str = \"\\n\".join([f\"{col}:{dtype}\" for col, dtype in structure])\n",
    "        files_str = \"\\n\".join(files)\n",
    "        \n",
    "        row = [group_number, num_columns, unique_dtypes, structure_str, files_str]\n",
    "        table_data.append(row)\n",
    "\n",
    "    # Afficher le tableau\n",
    "    print(tabulate(table_data, headers=table_headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appliquer une structure à un dataframe\n",
    "def apply_structure_to_df(df, structure):\n",
    "    \n",
    "    struct_cols = []\n",
    "    for col, _ in structure:\n",
    "        struct_cols.append(col)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col not in struct_cols:\n",
    "            df.drop(col, axis= 1, inplace= True)\n",
    "    \n",
    "    for col, dtype in structure:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.Series(dtype=dtype) \n",
    "        else:\n",
    "            if pd.api.types.is_integer_dtype(dtype):\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('int64')\n",
    "            elif pd.api.types.is_float_dtype(dtype):\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0).astype('float64')\n",
    "            else:\n",
    "                df[col] = df[col].astype(dtype) \n",
    "    \n",
    "    df = df.reindex(columns= struct_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choisir arbitrairement les structures à garder (les plus récentes)\n",
    "#j'avais fais un système de séléction automatique mais c'est une très mauvaise idée...\n",
    "target_df_caract= read_csv_file('data/raw/caracteristiques-2022.csv')[0]\n",
    "target_df_caract.rename(columns= {'Accident_Id':'Num_Acc'}, inplace= True)\n",
    "\n",
    "target_df_lieux= read_csv_file('data/raw/lieux-2022.csv')[0]\n",
    "target_df_usagers= read_csv_file('data/raw/usagers-2022.csv')[0]\n",
    "target_df_vehicules= read_csv_file('data/raw/vehicules-2022.csv')[0]\n",
    "\n",
    "target_df_list = [get_df_structure(target_df_caract), get_df_structure(target_df_lieux), get_df_structure(target_df_usagers), get_df_structure(target_df_vehicules)]\n",
    "\n",
    "#remplacer\n",
    "filename, df = next(iter(dataframes[0][-1].items()))\n",
    "df.rename(columns= {'Accident_Id':'Num_Acc'}, inplace= True)\n",
    "\n",
    "#brutalement \n",
    "i= 0\n",
    "for df_list in dataframes:\n",
    "    for item in df_list:\n",
    "        for filename, df in item.items():\n",
    "            df = apply_structure_to_df(df, target_df_list[i])\n",
    "    i+= 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouper après traitement pour résultat\n",
    "display_grouped_data(group_dataframes_by_struct(dataframes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on va tenter une concaténation sur tous les fichiers\n",
    "full_datasets = []\n",
    "\n",
    "for i in range(0, 4, 1):\n",
    "    full_datasets.append(pd.DataFrame()) \n",
    "\n",
    "i= 0\n",
    "for df_list in dataframes:\n",
    "    for item in df_list:\n",
    "        for filename, df in item.items():\n",
    "            full_datasets[i] = pd.concat([df, full_datasets[i]])\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in full_datasets:\n",
    "    display(df.dtypes)\n",
    "    display(df.head())\n",
    "    display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tentative pour faire un merge\n",
    "merged_df = full_datasets[2]\n",
    "\n",
    "\n",
    "for df_index, df in enumerate(full_datasets, start= 0):\n",
    "    if df_index != 2:\n",
    "        merged_df= pd.merge(merged_df, df, on= 'Num_Acc', how= 'left')\n",
    "\n",
    "merged_df.drop_duplicates(subset= 'id_usager')\n",
    "merged_df.set_index('id_usager', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(merged_df.head())\n",
    "display(merged_df.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
